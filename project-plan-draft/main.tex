\documentclass[letterpaper,11pt]{article}

\usepackage{titling}
\usepackage{graphicx}
\usepackage[numbers]{natbib}


% Define custom title format
\renewcommand{\maketitle}{
  \begin{center}
    {\LARGE\textbf{Comparison of Machine Learning Algorithms}}\par
    {\Large Project Plan}\par
    \vspace{10pt}
    Nathan Kurien\par
    CS3821: BSc Final Project \par
    \hrule % Horizontal line
    \vspace{20pt}
    \includegraphics[width=0.5\linewidth]{logo-large-cropped.png}
    \vspace{30pt}
    \begin{tabular}{c}
      Supervised by: Prof. Zhiyuan Luo \\
      Department of Computer Science \\
      Royal Holloway University of London
    \end{tabular}
  \end{center}
  \newpage
}

\title{Project Plan: Comparison of Machine Learning Algorithms}
\author{Nathan Kurien}
\date{24/09/2023} % Date? 

\begin{document}

\maketitle

\section{Abstract}
% Your abstract goes here.
% Provide an overview of the aims and objectives for the project.

Machine learning (ML) has taken the world by storm in recent times. In the era of unprecedented data generation, ML has emerged as a pivotal technology with profound implications for industries and everyday life. Supervised Learning algorithms exist to solve regression and classification tasks, and this project will focus primarily on classification. Within the ML landscape, classification algorithms play a critical role in tasks such as image recognition~\cite{imgdetectiondeepl}, spam filtering~\cite{spamfiltering}, medical diagnosis~\cite{diabetesclassification}, and more.\par
Classification~\cite{classificationreviewkotsiantis,mlreviewduttonconroy} problems involve using an algorithm, and a given set of inputs used as trained features, to classify any given input with a predefined label. In other words, Classification problems involve searching for a discrete target value, often among a subset of discrete values, whereas Regression~\cite{mlreviewduttonconroy} problems involve finding a continuous target value. \par
This project will involve an implementation of the K-Nearest Neighbour~\cite{NNCoverHart} algorithm, widely known as one of the simplest~\cite{simpleNN} learning algorithms. This algorithm will run parallel with an implementation of the Decision Tree~\cite{autodectreesmurthy, dectreeCharbuty, CARTBreiman} algorithm, another relatively simple algorithm with graspable interpretability and easy possibilities for visualisation. Various tree implementations exist, but I intend to focus on the Classification variation of the CART algorithm~\cite{CARTBreiman}, using Gini Impurity~\cite[p.309-312]{tEoSLHastie} as a goodness measure. Both algorithms will be evaluated and compared in their performance to tackle classification problems. \par
Focusing on these simplest examples of supervised learning classifiers, I aim to examine their application, performance and caveats. I hope to use performance metrics, such as accuracy, precision and F1 scores~\cite{classificationassessmenttharwat}, to evaluate the bias and variance of these algorithms. I also hope to extend these evaluations by adding alterations to my algorithms and using more complex datasets. These metrics will be explored to also evaluate the bias-variance tradeoff~\cite{biasvariancedomingos} of these algorithms. \par
I plan to use model validation techniques such as hold-out tests and cross-validation to determine these metrics. I aim to experiment with k-fold cross-validation~\cite[p.241-249]{tEoSLHastie} in particular, and establish the metrics mentioned during each iteration, and deduce the bias and variance present in the algorithms being assessed. \par
As a further study, I also plan to investigate the robustness of the K-Nearest Neighbours model by evaluating the consistency of performance over various folds during cross-validation. Within the context of decision trees, feature importance will be explored while investigating the decrease in impurity during validation of the model. \par

The models will ultimately be implemented in Python~\cite{python3}, creating the data structures using Lists and NumPy~\cite{numpy}, and using various existing libraries as points of reference. Sci-kit Learn~\cite{scikit-learn} is a highly regarded Python library by data scientists and includes various implementations of the models I hope to build. I will initiate research in this project by utilising this library to further my understanding of how these models function and how they should be evaluated. \par
Jupyter notebooks~\cite{jupyter} will be used initially while the models are in development. They will also be tremendously useful for building visualisations and validating the models. Later on in development, I hope to implement these models in a standalone interface that can take datasets as inputs and provide useful insights on the performance of chose models on this data. Creating the interface will potentially be implemented using Python libraries such as PyQt5~\cite{pyqt5}. There are vast amounts of resources I've found helpful in deciphering the theory behind this project. \par
I intend on using openly available datasets found on the UCI Repository~\cite{uci}, including the Delve datasets specified for Classification, and this may be sufficient to evaluate the performance of these chosen algorithms. If I choose to extend further, I could use datasets from open-source communities such as Kaggle~\cite{kaggle}, OpenML~\cite{OpenML2013} and huggingface~\cite{huggingface} for further training. Datasets from the latter are likely to require more preprocessing and normalisation.  \par
Within this project, I hope to uncover and harness skills within data mining and machine learning that are proving invaluable and immensely relevant in the current climate of technological advancement. This project aims to delve into the core fundamentals of statistical learning and optimisiing such techniques to find valuable insights. Within industry, as investment into this field skyrockets, I believe that insights made in this project will be valuable in many applications in industry. 


\newpage
\section{Milestones}
Listed below are a set of deliverables I hope to undertake in the process of putting together my project. \par 
Along with reports and programs, I've included a success criteria section for both terms. I've noticed while planning this project that it's quite easy to get carried away with theory and dive deeper and deeper into algorithms without focusing on implementation. In the success criteria, I hope to clearly define what is essential to deliver in order for me to deliver a working project in due time. 
\subsection{First Term}
\subsubsection{Reports}
% Describe the milestones for the first term, including reports.
Report 1: Nearest Neighbour Algorithms - In this report I hope to discuss the theory behind nearest neighbour algorithms, 1-Nearest Neighbour and K-Nearest Neighbour, and further details behind how they function. \par 
Report 2: Decision Tree Algorithms - In this report, I hope to discuss the theory behind the Decision Tree algorithm and explore its measures of uniformity. \par

\subsubsection{Programs}
% Describe the milestones for the first term, including programs.
Proof of Concept Programs in Python

\subsubsection{Success Criteria}
By the end of the first term, I should have implemented two algorithms
\subsubsection{Further Extensions}
(Extensions on metrics and SVMs) Ensemble Learning? Kernel Methods?

\subsection{Second Term}
\subsubsection{Reports}
% Describe the milestones for the second term, including reports.
[Details of second-term report milestones]

\subsubsection{Programs}
% Describe the milestones for the second term, including programs.
[Details of second-term program milestones]

\subsubsection{Success Criteria}

\section{Risks and Mitigations}
% Discuss potential risks associated with your project and how you plan to mitigate them.
Even best-laid plans often go astray, and in this section I hope to list possible difficulties I may face as I undertake this project, along with contingency plans and mitigations in the hopes to reduce great losses.


\section{Planning and Time-Scales}
% Provide details on the planning and time-scales for your project.
[Project planning and time-scales]
\bibliographystyle{IEEEtranN}
\bibliography{bibliography.bib}

\end{document}
