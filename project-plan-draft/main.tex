\documentclass[report,10pt]{article}

\usepackage{titling}
\usepackage{graphicx}
\usepackage[numbers]{natbib}


% Define custom title format
\renewcommand{\maketitle}{
  \begin{center}
    {\LARGE\textbf{Comparison of Machine Learning Algorithms}}\par
    {\Large Project Plan}\par
    \vspace{10pt}
    Nathan Kurien\par
    CS3821: BSc Final Project \par
    \hrule % Horizontal line
    \vspace{20pt}
    \includegraphics[width=0.5\linewidth]{logo-large-cropped.png}
    \vspace{30pt}
    \begin{tabular}{c}
      Supervised by: Prof. Zhiyuan Luo \\
      Department of Computer Science \\
      Royal Holloway University of London
    \end{tabular}
  \end{center}
  \pagebreak
}

\title{Project Plan: Comparison of Machine Learning Algorithms}
\author{Nathan Kurien}
\date{24/09/2023} % Date? 

\begin{document}

\maketitle

\section{Abstract}
% Your abstract goes here.
% Provide an overview of the aims and objectives for the project.

Machine learning (ML) has taken the world by storm in recent times. In the era of unprecedented data generation, ML has emerged as a pivotal technology with profound implications for industries and everyday life. Supervised Learning algorithms exist to solve regression and classification tasks, and this project will focus primarily on classification. Within the ML landscape, classification algorithms play a critical role in tasks such as image recognition~\cite{imgdetectiondeepl}, spam filtering~\cite{spamfiltering}, medical diagnosis~\cite{diabetesclassification}, and more.\par
Classification~\cite{classificationreviewkotsiantis,mlreviewduttonconroy} problems involve using an algorithm, and a given set of inputs used as trained features, to classify any given input with a predefined label. In other words, Classification problems involve searching for a discrete target value, often among a subset of discrete values, whereas Regression~\cite{mlreviewduttonconroy} problems involve finding a continuous target value. \par
This project will involve an implementation of the K-Nearest Neighbour~\cite{NNCoverHart} algorithm, widely known as one of the simplest~\cite{simpleNN} learning algorithms. This algorithm will run parallel with an implementation of the Decision Tree~\cite{autodectreesmurthy, dectreeCharbuty, CARTBreiman} algorithm, another relatively simple algorithm with graspable interpretability and easy possibilities for visualisation. Various tree implementations exist, but I intend to focus on the Classification variation of the CART algorithm~\cite{CARTBreiman}, using Gini Impurity~\cite[p.309-312]{tEoSLHastie} as a goodness measure. Both algorithms will be evaluated and compared in their performance to tackle classification problems. \par
Focusing on these simplest examples of supervised learning classifiers, I aim to examine their application, performance and caveats. I hope to use performance metrics, such as accuracy, precision and F1 scores~\cite{classificationassessmenttharwat}, to evaluate the bias and variance of these algorithms. I also hope to extend these evaluations by adding alterations to my algorithms and using more complex datasets. These metrics will be explored to also evaluate the bias-variance tradeoff~\cite{biasvariancedomingos} of these algorithms. \par
I plan to use model validation techniques such as hold-out tests and cross-validation~\cite{crossvalidationKohavi} to determine these metrics. I aim to experiment with k-fold cross-validation~\cite[p.241-249]{tEoSLHastie} in particular, and establish the metrics mentioned during each iteration, and deduce the bias and variance present in the algorithms being assessed. \par
As a further study, I also plan to investigate the robustness of the K-Nearest Neighbours model by evaluating the consistency of performance over various folds during cross-validation. Within the context of decision trees, feature importance will be explored while investigating the decrease in impurity during validation of the model. \par

The models will ultimately be implemented in Python~\cite{python3}, creating the data structures using Lists and NumPy~\cite{numpy}, and using various existing libraries as points of reference. Sci-kit Learn~\cite{scikit-learn} is a highly regarded Python library by data scientists and includes various implementations of the models I hope to build. I will initiate research in this project by utilising this library to further my understanding of how these models function and how they should be evaluated. \par
Jupyter notebooks~\cite{jupyter} will be used initially while the models are in development. They will also be tremendously useful for building visualisations and validating the models. Later on in development, I hope to implement these models in a standalone interface that can take datasets as inputs and provide useful insights on the performance of chose models on this data. Creating the interface will potentially be implemented using Python libraries such as PyQt5~\cite{pyqt5}. There are vast amounts of resources I've found helpful in deciphering the theory behind this project. \par
I intend on using openly available datasets found on the UCI Repository~\cite{uci}, including the Delve datasets specified for Classification, and this may be sufficient to evaluate the performance of these chosen algorithms. If I choose to extend further, I could use datasets from open-source communities such as Kaggle~\cite{kaggle}, OpenML~\cite{OpenML2013} and huggingface~\cite{huggingface} for further training. Datasets from the latter are likely to require more preprocessing and normalisation.  \par
Within this project, I hope to uncover and harness skills within data mining and machine learning that are proving invaluable and immensely relevant in the current climate of technological advancement. This project aims to delve into the core fundamentals of statistical learning and optimisiing such techniques to find valuable insights. Within industry, as investment into this field skyrockets, I believe that insights made in this project will be valuable in many applications in industry. 


\pagebreak


\section{Milestones}
Listed below are a set of deliverables I hope to undertake in the process of putting together my project. \par 
Along with reports and programs, I've included a success criteria section for both terms. I've noticed while planning this project that it's quite easy to get carried away with theory and dive deeper and deeper into algorithms without focusing on implementation. In the success criteria, I hope to clearly define what is essential to deliver in order for me to deliver a working project in due time. 
\subsection{First Term}
\subsubsection{Reports}
% Describe the milestones for the first term, including reports.
Report 1: Nearest Neighbour Algorithms - In this report I hope to discuss the theory behind nearest neighbour algorithms, 1-Nearest Neighbour and K-Nearest Neighbour, and further details behind how they function. \par 
Report 2: Decision Tree Algorithms - In this report, I hope to discuss the theory behind the Decision Tree algorithm and explore its measures of uniformity. \par
I sense that completing both of these reports will fulfill my understanding of the inner workings of these algorithms, and will supplement my interim report at the end of the year. \par

\subsubsection{Programs}
% Describe the milestones for the first term, including programs.
I will begin by implementing Proof of Concept programs in Python, starting very simple, and then expanding as I begin to expand both on the algorithms, and the complexity of my datasets.

\subsubsection{Success Criteria}
By the end of the first term, I should have implemented two algorithms
\subsubsection{Further Extensions}
(Extensions on metrics and SVMs) Ensemble Learning? Kernel Methods?

\subsection{Second Term}
\subsubsection{Reports}
% Describe the milestones for the second term, including reports.
[Details of second-term report milestones]

\subsubsection{Programs}
% Describe the milestones for the second term, including programs.
[Details of second-term program milestones]

\subsubsection{Success Criteria}
\pagebreak
\section{Planning and Time-Scales}
% Provide details on the planning and time-scales for your project.
Now that we've covered various deliverables and milestones to mark my progress with the project, this section will cover the projected timeline of when these items will be delivered, with a balance of caution and optimism.
\pagebreak
\section{Risks and Mitigations}
% Discuss potential risks associated with your project and how you plan to mitigate them.
Even best-laid plans often go astray, and in this section, I hope to list possible difficulties I may face as I undertake this project, along with contingency plans and mitigations in the hopes of reducing great losses.
\subsection{Overemphasis on Theory and Research}

The most apparent challenge I've found during the process of putting this plan together has often been the tendency to get lost in deciphering and understanding complicated, though interesting, theory behind the project's background - as well as looking into possible extensions before the core deliverables have even been put into place. \par
This challenge is moreso a feature of my own tendencies rather than the project itself, it is tempting to read and research rather than deliver what the project is demanding - and what the project is demanding is relatively simple compared to the various extensions that are possible. \par
I sense that the likelihood of this causing issues is high considering it's already had an effect on progress, I hope to mitigate this by reviewing my progress often during the weekly reviews and in my diary entries. I've found that the use of my diary to look at the overall scope of my project, as well as upcoming deadlines, has been useful in pulling me out of theory-induced rabbit holes. 

\subsection{Data Preprocessing Challenges}

One of the most common and time-consuming challenges in any machine learning-related project is said to be related to data preprocessing and handling with datasets used in the project. \par
It's never clear how much time this may take, but it'll be necessary to mitigate this by simply allocating a reasonable amount of planned time to focusing on this, as well as ensuring that handling missing features in data is an aspect that is considered during the software design process. \par
For the sake of simplicity and sanity, I will initially focus my PoC programs on well-known and used UCI Respository~\cite{uci} datasets, that will be likely easier to handle and manage - as well as many resources available if I find any issues. It's important that I focus on the core purpose of this project in evaluating these models, rather than waste time and resources handling data-related hiccups, so I will focus on simple datasets at first before diving into complicated datasets. Further into the project, I could also implement outlier handling in my project, though it may not be vital, it's something worth considering while designing my software. \par

\subsection{Software Complexity}
Building these algorithms from scratch can quite easily become a convoluted process without the correct approach applied. \par
I intend to use Object-Oriented Programming techniques where applicable to give the software a relatively clear architecture that can be built upon and extended effectively. I intend to create UML diagrams to outline the structure of the models, as well as how data is handled by the model. Where applicable, I hope to use unit test programs, as is standard in Test Driven Development, upon the models to keep track of functionality as more and more subroutines are added over time. \par
Making effective use of my GitLab repository will be helpful in staying on top of software complexity too, I will likely make use of branches when extending my algorithms' functionality. \par
\subsection{GUI Challenges}
\subsection{Data Availability and Quality}
\subsection{Project Scope Creep}
\subsection{Time Management}

\pagebreak
\bibliographystyle{IEEEtranN}
\bibliography{bibliography.bib}

\end{document}
