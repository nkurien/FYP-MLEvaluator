\documentclass[letterpaper,11pt]{article}

\usepackage{titling}
\usepackage{graphicx}
\usepackage[numbers]{natbib}


% Define custom title format
\renewcommand{\maketitle}{
  \begin{center}
    {\LARGE\textbf{Comparison of Machine Learning Algorithms}}\par
    {\Large Project Plan}\par
    \vspace{10pt}
    Nathan Kurien\par
    CS3821: BSc Final Project \par
    \hrule % Horizontal line
    \vspace{20pt}
    \includegraphics[width=0.5\linewidth]{logo-large-cropped.png}
    \vspace{30pt}
    \begin{tabular}{c}
      Supervised by: Prof. Zhiyuan Luo \\
      Department of Computer Science \\
      Royal Holloway University of London
    \end{tabular}
  \end{center}
  \newpage
}

\title{Project Plan: Comparison of Machine Learning Algorithms}
\author{Nathan Kurien}
\date{24/09/2023} % Date? 

\begin{document}

\maketitle

\section{Abstract}
% Your abstract goes here.
% Provide an overview of the aims and objectives for the project.

Machine learning (ML) has taken the world by storm in recent times. In the era of unprecedented data generation, ML has emerged as a pivotal technology with profound implications for industries and everyday life. Supervised Learning algorithms exist to solve regression and classification tasks, and this project will focus primarily on classification. Within the ML landscape, classification algorithms play a critical role in tasks such as image recognition~\cite{imgdetectiondeepl}, spam filtering~\cite{spamfiltering}, medical diagnosis~\cite{diabetesclassification}, and more.\par
Classification~\cite{classificationreviewkotsiantis,mlreviewduttonconroy} problems involve using an algorithm, and a given set of inputs used as trained features, to classify any given input with a predefined label. In other words, Classification problems involve searching for a discrete target value, often among a subset of discrete values, whereas Regression~\cite{mlreviewduttonconroy} problems involve finding a continuous target value. \par
This project will involve an implementation of the K-Nearest Neighbour~\cite{NNCoverHart} algorithm, widely known as one of the simplest~\cite{simpleNN} learning algorithms. This algorithm will run parallel with an implementation of the Decision Tree~\cite{autodectreesmurthy, dectreeCharbuty} algorithm, another relatively simple algorithm with graspable interpretability and easy possibilities for visualisation. Both algorithms will be evaluated and compared in their performance to tackle classification problems. \par
This project aims to focus on the simplest examples of supervised learning classifiers and examine their application, performance and caveats. I hope to use performance metrics, such as accuracy, precision and F1 scores, to evaluate the bias and variance of these algorithms. I also hope to extend these evaluations by adding alterations to my algorithms and using more complex datasets. \par
I intend on using openly available datasets found on the UCI Repository~\cite{uci}, including the Delve datasets, as well as datasets from open-source communities such as Kaggle, OpenML~\cite{OpenML2013} and huggingface for further training. Datasets from the latter are likely to require more preprocessing and normalisation. \par
Scikit-learn~\cite{scikit-learn} is a highly regarded Python~\cite{python3} library by data scientists and will be used frequently as a reference point in this project as I design my algorithms.


\newpage
\section{Milestones}
Listed below are a set of deliverables I hope to undertake in the process of putting together my project. \par 
Along with reports and programs, I've included a success criteria section for both terms. I've noticed while planning this project that it's quite easy to get carried away with theory and dive deeper and deeper into algorithms without focusing on implementation. In the success criteria, I hope to clearly define what is essential to deliver in order for me to deliver a working project in due time. 
\subsection{First Term}
\subsubsection{Reports}
% Describe the milestones for the first term, including reports.
Report 1: Nearest Neighbour Algorithms - In this report I hope to discuss the theory behind nearest neighbour algorithms, 1-Nearest Neighbour and K-Nearest Neighbour, and further details behind how they function. \par 
Report 2: Decision Tree Algorithms - In this report, I hope to discuss the theory behind the Decision Tree algorithm and explore its measures of uniformity. \par

\subsubsection{Programs}
% Describe the milestones for the first term, including programs.
Proof of Concept Programs in Python

\subsubsection{Success Criteria}
By the end of the first term, I should have implemented two algorithms
\subsubsection{Further Extensions}
(Extensions on metrics and SVMs) Ensemble Learning? Kernel Methods?

\subsection{Second Term}
\subsubsection{Reports}
% Describe the milestones for the second term, including reports.
[Details of second-term report milestones]

\subsubsection{Programs}
% Describe the milestones for the second term, including programs.
[Details of second-term program milestones]

\subsubsection{Success Criteria}

\section{Risks and Mitigations}
% Discuss potential risks associated with your project and how you plan to mitigate them.
Even best-laid plans often go astray, and in this section I hope to list possible difficulties I may face as I undertake this project, along with contingency plans and mitigations in the hopes to reduce great losses.


\section{Planning and Time-Scales}
% Provide details on the planning and time-scales for your project.
[Project planning and time-scales]
\bibliographystyle{IEEEtranN}
\bibliography{bibliography.bib}

\end{document}
